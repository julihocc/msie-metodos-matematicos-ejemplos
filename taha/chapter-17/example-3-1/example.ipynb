{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following transition matrix for a Markov chain with 3 states:\n",
    "```math \n",
    "P = \\begin{pmatrix}\n",
    "0.2 & 0.5 & 0.3 \\\\\n",
    "0 & 0.5 & 0.5 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}\n",
    "```\n",
    "\n",
    "The corresponding graph is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    1 -->|0.2| 1\n",
    "    1 -->|0.5| 2\n",
    "    1 -->|0.3| 3\n",
    "    2 -->|0.5| 2\n",
    "    2 -->|0.5| 3\n",
    "    3 -->|1| 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Markov chains, *absorbing* and *transient* states describe types of behaviors of states within the chain:\n",
    "\n",
    "1. **Absorbing State**: This is a state in the Markov chain from which, once entered, it cannot transition to any other state. Formally, if a state \\( i \\) is absorbing, then \\( P(i \\to i) = 1 \\) (the probability of staying in state \\( i \\) is 1), and there is no probability of transitioning to any other state. In the given Markov chain, state 3 is an absorbing state since it transitions to itself with probability 1.\n",
    "\n",
    "2. **Transient State**: A transient state is one that, starting from it, there is a nonzero probability that the process will eventually leave this state and not return. This means that the probability of eventually reaching an absorbing state or another type of state is 1. In the provided Markov chain, states 1 and 2 are transient states. For example:\n",
    "   - State 1 transitions to itself with a probability of 0.2, to state 2 with 0.5, and to state 3 with 0.3. Since there is a chance of reaching state 3 (the absorbing state), it is classified as transient.\n",
    "   - Similarly, state 2 transitions to itself with a probability of 0.5 and to state 3 with a probability of 0.5, meaning it can eventually reach state 3.\n",
    "\n",
    "In summary, an absorbing state like state 3 in this chain is a \"terminal\" state, while transient states like states 1 and 2 are intermediate stages that the process can pass through but will ultimately leave as it progresses towards absorption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "sage"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.200000000000000 0.500000000000000 0.300000000000000]\n",
       "[0.000000000000000 0.500000000000000 0.500000000000000]\n",
       "[0.000000000000000 0.000000000000000  1.00000000000000]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = Matrix([[.2, .5, .3], [0, .5, .5], [0,0,1]])\n",
    "P "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "sage"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.26765060022824e-70 1.31476817536835e-30     1.00000000000000]\n",
       "[   0.000000000000000 7.88860905221012e-31     1.00000000000000]\n",
       "[   0.000000000000000    0.000000000000000     1.00000000000000]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P**100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Markov Chains\n",
    "\n",
    "A **regular Markov chain** is a special type of Markov chain in which, after a certain number of transitions, it is possible to get from any state to any other state, regardless of the initial state. In other words, a regular Markov chain has a transition matrix where some power of this matrix has all positive entries, ensuring that every state is reachable from every other state after a sufficient number of steps.\n",
    "\n",
    "Here are the key characteristics of a regular Markov chain:\n",
    "\n",
    "1. **Irreducibility**: In a regular Markov chain, it is possible to reach any state from any other state, implying that the system has no isolated states.\n",
    "  \n",
    "2. **Aperiodicity**: The chain does not follow strict periodic cycles. For each state, the greatest common divisor of the number of steps that can return the system to that state must be 1.\n",
    "\n",
    "3. **Convergence to Steady-State**: Regular Markov chains have a unique steady-state distribution, meaning that as the number of steps goes to infinity, the state probabilities converge to a unique stationary distribution that is independent of the initial state.\n",
    "\n",
    "These properties make regular Markov chains well-suited for modeling processes that eventually stabilize or converge, such as queuing systems, certain population dynamics, or long-term market predictions【26†source】【43†source】."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example \n",
    "\n",
    "Let's walk through the weather prediction example for a regular Markov chain in detail.\n",
    "\n",
    "### Example: Weather Prediction Model\n",
    "\n",
    "The weather can either be **sunny** (S) or **rainy** (R). The transition probabilities are provided by the following transition matrix:\n",
    "\n",
    "$$\n",
    "P = \\begin{bmatrix}\n",
    "0.8 & 0.2 \\\\\n",
    "0.4 & 0.6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- The first row corresponds to **sunny** weather, and the second row to **rainy** weather.\n",
    "- Each value represents the probability of transitioning to another state (or remaining in the same state) based on the current state:\n",
    "  - If today is **sunny** (S), there’s an 80% chance it will be sunny tomorrow, and a 20% chance it will be rainy.\n",
    "  - If today is **rainy** (R), there’s a 40% chance it will be sunny tomorrow, and a 60% chance it will be rainy.\n",
    "\n",
    "### Step 1: Transition Matrix Powers\n",
    "\n",
    "To check if this Markov chain is regular, we calculate powers of the transition matrix to see if eventually all entries are positive.\n",
    "\n",
    "Calculating $P^2$, the transition matrix for two steps:\n",
    "\n",
    "$$\n",
    "P^2 = P \\times P = \\begin{bmatrix}\n",
    "0.8 & 0.2 \\\\\n",
    "0.4 & 0.6\n",
    "\\end{bmatrix} \\times \\begin{bmatrix}\n",
    "0.8 & 0.2 \\\\\n",
    "0.4 & 0.6\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.72 & 0.28 \\\\\n",
    "0.56 & 0.44\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The matrix $P^2$ still has positive entries, which means it's possible to transition between states over two steps.\n",
    "\n",
    "### Step 2: Checking Regularity\n",
    "\n",
    "For a Markov chain to be regular, some power of the transition matrix must have all positive entries. In this case, both $P$ and $P^2$ already have all positive entries. Therefore, this is a **regular Markov chain**.\n",
    "\n",
    "### Step 3: Steady-State Distribution\n",
    "\n",
    "A regular Markov chain will converge to a **steady-state distribution**, where the probabilities don’t change after further transitions. The steady-state vector $\\pi$ satisfies the equation:\n",
    "\n",
    "$$\n",
    "\\pi P = \\pi\n",
    "$$\n",
    "\n",
    "This is equivalent to solving the system:\n",
    "\n",
    "```math\n",
    "\\pi_1 = 0.8 \\pi_1 + 0.4 \\pi_2\n",
    "\\pi_2 = 0.2 \\pi_1 + 0.6 \\pi_2\n",
    "\\pi_1 + \\pi_2 = 1\n",
    "```\n",
    "\n",
    "Substituting the third equation ($\\pi_1 + \\pi_2 = 1$) into the first two, we can solve for the steady-state probabilities:\n",
    "\n",
    "$$\n",
    "\\pi_1 = 0.67, \\quad \\pi_2 = 0.33\n",
    "$$\n",
    "\n",
    "Thus, in the long run, the weather will be **sunny** 67% of the time and **rainy** 33% of the time, no matter the initial state.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This simple weather prediction model forms a regular Markov chain. Over time, the probabilities of sunny and rainy weather stabilize and converge to a steady-state distribution, where sunny weather occurs 67% of the time, and rainy weather occurs 33% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ergodic Markov Chains\n",
    "\n",
    "An **ergodic Markov chain** is a special type of Markov chain that has certain properties ensuring it will converge to a unique steady-state distribution regardless of the initial state. For a Markov chain to be **ergodic**, it must meet the following conditions:\n",
    "\n",
    "### 1. **Irreducibility**\n",
    "A Markov chain is **irreducible** if it is possible to get from any state to any other state (possibly in multiple steps). In other words, for every pair of states \\( i \\) and \\( j \\), there exists some number of steps \\( n \\) such that the probability of transitioning from state \\( i \\) to state \\( j \\) in \\( n \\) steps is greater than zero.\n",
    "\n",
    "This ensures that no state is isolated and that the entire system communicates, meaning that all states are accessible from any other state.\n",
    "\n",
    "### 2. **Aperiodicity**\n",
    "A Markov chain is **aperiodic** if it does not follow a cyclic pattern, i.e., the chain does not return to a state in fixed periodic intervals. More formally, for any state \\( i \\), the greatest common divisor of the number of steps it takes to return to that state is 1. \n",
    "\n",
    "This condition ensures that the chain does not get \"stuck\" in cycles and can return to any state at irregular intervals.\n",
    "\n",
    "### 3. **Positive Recurrence**\n",
    "A state is **positive recurrent** if the expected number of steps to return to that state is finite. In an ergodic Markov chain, all states are positive recurrent, meaning that starting from any state, the process is guaranteed to return to that state in a finite expected time.\n",
    "\n",
    "### **Key Properties of Ergodic Markov Chains:**\n",
    "- **Convergence to a Steady-State Distribution**: Regardless of the initial state, an ergodic Markov chain will always converge to the same unique steady-state distribution over time. This distribution does not depend on where the process started.\n",
    "  \n",
    "- **Long-Term Behavior**: In the long run, the proportion of time spent in each state will converge to the values given by the steady-state distribution. This is sometimes called the **ergodic theorem** for Markov chains.\n",
    "\n",
    "### Difference from a Regular Markov Chain\n",
    "- **Regular Markov Chains** are always ergodic because they fulfill both the irreducibility and aperiodicity conditions (and positive recurrence naturally follows in finite state spaces).\n",
    "- However, **ergodic** Markov chains are more general than regular Markov chains because they include cases where some states may take a long but finite time to transition, as long as they eventually do. Not all ergodic chains are regular.\n",
    "\n",
    "### Example: Ergodic Chain\n",
    "In the weather example you gave earlier, the weather Markov chain is **ergodic** because:\n",
    "1. **Irreducibility**: It is possible to transition from sunny to rainy and vice versa (even if it might take multiple steps).\n",
    "2. **Aperiodicity**: There is no fixed cycle or period in which the weather alternates. \n",
    "3. **Positive Recurrence**: Both sunny and rainy states can be reached within a finite number of steps.\n",
    "\n",
    "Thus, the Markov chain converges to a steady-state distribution where the probabilities stabilize (67% sunny, 33% rainy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counterexample: Ergodic but Not Regular Markov Chain\n",
    "\n",
    "An example of an **ergodic Markov chain** that is **not regular** can be constructed using an infinite state space. In finite-state spaces, an ergodic chain is always regular, but in infinite-state spaces, we can find examples that are ergodic but not regular.\n",
    "\n",
    "### Example: Ergodic but Not Regular Markov Chain\n",
    "\n",
    "Consider a **Markov chain on the set of positive integers** \\( \\{1, 2, 3, 4, \\dots \\} \\) with the following transition rules:\n",
    "\n",
    "- From state \\( n \\), the chain moves to state \\( n-1 \\) with probability 1, unless it is at state 1.\n",
    "- When at state 1, the chain has a probability 1 of staying in state 1.\n",
    "\n",
    "The transition matrix \\( P \\) can be described as follows:\n",
    "\n",
    "\\[\n",
    "P = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & 0 & \\dots \\\\\n",
    "1 & 0 & 0 & 0 & 0 & \\dots \\\\\n",
    "0 & 1 & 0 & 0 & 0 & \\dots \\\\\n",
    "0 & 0 & 1 & 0 & 0 & \\dots \\\\\n",
    "0 & 0 & 0 & 1 & 0 & \\dots \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\ddots \n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "### Why This Markov Chain is **Ergodic**:\n",
    "1. **Irreducibility**: From any state \\( n \\), the chain can eventually reach state 1. Once in state 1, it stays there, so it is possible to get from any state to state 1, fulfilling the irreducibility condition.\n",
    "   \n",
    "2. **Aperiodicity**: The chain is aperiodic because there is no fixed period with which the chain cycles between states. In particular, once the chain reaches state 1, it remains in state 1 with probability 1, breaking any periodic pattern.\n",
    "\n",
    "3. **Positive Recurrence**: Starting from any state \\( n \\), the chain will eventually reach state 1 in a finite number of steps. Once in state 1, the chain stays there forever. Therefore, the state 1 is positive recurrent, and all other states are transient but lead to state 1 in a finite time.\n",
    "\n",
    "### Why This Markov Chain is **Not Regular**:\n",
    "A **regular** Markov chain requires that for some power of the transition matrix \\( P^k \\), all entries must be positive. In this case, the transition matrix has many zero entries, even in powers of the matrix. For instance, if the chain starts at any state \\( n \\), it takes exactly \\( n-1 \\) steps to reach state 1, and it can only move downward in a deterministic way (from \\( n \\) to \\( n-1 \\), then from \\( n-1 \\) to \\( n-2 \\), and so on).\n",
    "\n",
    "Thus, even for large powers of the matrix, there are zero probabilities for transitions between many pairs of states. For example, there is no probability of transitioning from state \\( 2 \\) to state \\( 4 \\) in any number of steps, which violates the requirement of all-positive entries in a regular chain.\n",
    "\n",
    "### Summary:\n",
    "This Markov chain is **ergodic** because:\n",
    "- It is possible to reach state 1 from any state, and once there, the chain stays at state 1 (irreducible and aperiodic).\n",
    "- It has a unique limiting distribution (in this case, all probability mass concentrates on state 1).\n",
    "\n",
    "However, it is **not regular** because the transition matrix does not have positive probabilities for transitions between all pairs of states, even for large powers of the matrix.\n",
    "\n",
    "This illustrates that an infinite-state Markov chain can be ergodic without being regular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sage"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 9.5",
   "language": "sage",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
